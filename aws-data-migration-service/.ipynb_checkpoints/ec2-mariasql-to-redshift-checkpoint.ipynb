{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6810ac77-b861-49bf-aba7-e32e8cf10ecb",
   "metadata": {},
   "source": [
    "Okay, let's dive into using AWS Database Migration Service (DMS) for both an initial full data load and ongoing change data capture (CDC) for your MariaDB database on an EC2 instance to Redshift Serverless. DMS is a good option for this use case because it handles both parts of the problem relatively well.\n",
    "\n",
    "Hereâ€™s a step-by-step guide on how to set this up:\n",
    "\n",
    "**Prerequisites:**\n",
    "\n",
    "1.  **AWS Account:** You need an active AWS account.\n",
    "2.  **EC2 Instance with MariaDB:** You must have a MariaDB database running on an EC2 instance.\n",
    "3.  **Redshift Serverless:** You should have an active Redshift Serverless endpoint configured.\n",
    "4.  **Security Groups:** Ensure your security groups allow connectivity between your EC2 instance and AWS services, as well as between DMS and Redshift.\n",
    "5.  **IAM Roles:** You need IAM roles for DMS to access your EC2 instance, MariaDB, and Redshift Serverless.\n",
    "6.  **MariaDB Binary Logging Enabled:** Your MariaDB instance needs to have binary logging (binlogs) enabled, as this is how DMS will capture changes.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "**1. Set Up MariaDB for Change Data Capture (CDC):**\n",
    "\n",
    "*   **Enable Binlogs:** Edit your MariaDB configuration file (typically `my.cnf` or `my.ini`) to enable binary logging. Add or modify the following lines:\n",
    "\n",
    "    ```\n",
    "    log_bin = mysql-bin\n",
    "    binlog_format = ROW\n",
    "    server_id = 1  #  Set a unique server_id integer\n",
    "    ```\n",
    "\n",
    "*   **Restart MariaDB:** After making the changes, restart your MariaDB service.\n",
    "*   **Verify Binlog Enabled:** You can verify binary logging by running the following SQL query:\n",
    "\n",
    "    ```sql\n",
    "    SHOW VARIABLES LIKE 'log_bin';\n",
    "    ```\n",
    "\n",
    "    It should show `log_bin = ON`.\n",
    "\n",
    "**2. Create IAM Roles for DMS:**\n",
    "\n",
    "*   **DMS Service Role:** Create an IAM role that DMS can assume. This role should have policies that allow DMS to:\n",
    "    *   Connect to your EC2 instance.\n",
    "    *   Connect to your MariaDB database.\n",
    "    *   Connect to your Redshift Serverless endpoint.\n",
    "    *   Read/write data to and from S3 (if you choose to use S3 as an intermediary).\n",
    "\n",
    "*   **Example Policies:** You'll need to create policies like:\n",
    "    *   `AWSLambdaBasicExecutionRole` (for logging)\n",
    "    *   `AmazonS3FullAccess` (if using S3)\n",
    "    *   Permissions for CloudWatch Logs.\n",
    "    *   Permissions to connect and operate with MariaDB and Redshift.\n",
    "\n",
    "**3. Create the DMS Replication Instance:**\n",
    "\n",
    "*   Go to the AWS DMS console.\n",
    "*   Choose **Replication instances** and click **Create replication instance.**\n",
    "*   Configure the replication instance:\n",
    "    *   **Name:** Give it a name.\n",
    "    *   **Instance class:** Choose an appropriate instance class for your workload.\n",
    "    *   **Engine version:** Select the appropriate DMS engine version.\n",
    "    *   **VPC:** Choose the VPC where your EC2 instance is located.\n",
    "    *   **Multi-AZ:** Choose whether to use Multi-AZ for high availability.\n",
    "    *   **Allocate Storage:** Assign sufficient storage to hold intermediate data.\n",
    "    *   **Publicly Accessible:** If you do not have a VPC, or are testing, you can make the instance public, otherwise, use private.\n",
    "*   Click **Create replication instance.** This process may take a few minutes.\n",
    "\n",
    "**4. Create DMS Endpoints:**\n",
    "\n",
    "*   **Source Endpoint (MariaDB):**\n",
    "    *   Go to **Endpoints** in the DMS console.\n",
    "    *   Choose **Create endpoint.**\n",
    "    *   **Endpoint Type:** Source.\n",
    "    *   **Engine:** MySQL/MariaDB.\n",
    "    *   **Server Name:** Enter the private IP address or hostname of your EC2 instance.\n",
    "    *   **Port:** 3306 (or the port your MariaDB is listening on).\n",
    "    *   **User name/Password:** Enter your MariaDB credentials.\n",
    "    *   **Database Name:** Enter the name of your database.\n",
    "    *   **Security:** Set the VPC and security groups for connecting to the EC2 instance.\n",
    "    *   **Test connection:** Test the connection to ensure it's working.\n",
    "\n",
    "*   **Target Endpoint (Redshift Serverless):**\n",
    "    *   Choose **Create endpoint** again.\n",
    "    *   **Endpoint Type:** Target.\n",
    "    *   **Engine:** Amazon Redshift.\n",
    "    *   **Server Name:** Enter the Redshift Serverless endpoint name.\n",
    "    *   **Port:** 5439 (the default Redshift port).\n",
    "    *   **User Name/Password:** Enter the Redshift credentials.\n",
    "    *   **Database Name:** Enter the name of your Redshift database.\n",
    "    *   **Security:** Set the VPC and security groups for connecting to Redshift.\n",
    "    *   **Test connection:** Test the connection to ensure it's working.\n",
    "\n",
    "**5. Create a DMS Task:**\n",
    "\n",
    "*   Go to **Database migration tasks** and click **Create task.**\n",
    "*   **Task Settings:**\n",
    "    *   **Task identifier:** Provide a unique task name.\n",
    "    *   **Replication instance:** Select the replication instance you created.\n",
    "    *   **Source endpoint:** Select your MariaDB endpoint.\n",
    "    *   **Target endpoint:** Select your Redshift Serverless endpoint.\n",
    "    *   **Migration Type:** Choose **Migrate existing data and replicate ongoing changes.**\n",
    "    *   **Target table preparation mode:**\n",
    "      *   Choose \"Do nothing\" if the tables already exist in Redshift.\n",
    "      *   Choose \"Drop tables on target\" if you want DMS to drop and recreate the target tables.\n",
    "      *   Choose \"Truncate tables on target\" to truncate existing data.\n",
    "    *   **Table mapping:**\n",
    "      *   Use \"Schema name\" to map by schema name (select the source schema/database in the filter).\n",
    "      *   Use \"Table name\" to map specific tables. You can choose the schemas and tables you want to include in the migration.\n",
    "\n",
    "*   **Task Settings (Advanced):**\n",
    "    *   **Enable logging:** You should enable logging to capture any errors.\n",
    "    *   **LOB handling:** Configure how to handle large objects if any exist.\n",
    "    *   **CDC Settings:**\n",
    "      *   Keep the defaults for general CDC.\n",
    "*   Click **Create task.**\n",
    "*   **Start Task:** Once created, select the task and click **Start/Resume**.\n",
    "\n",
    "**6. Monitor the Migration:**\n",
    "\n",
    "*   DMS will first perform a full load of your data to Redshift.\n",
    "*   After the full load is completed, it will start replicating changes from the MariaDB binlogs.\n",
    "*   You can monitor the progress of your task in the DMS console. Check the task logs for any errors.\n",
    "*   Use CloudWatch to monitor the replication instance, task, and network metrics.\n",
    "\n",
    "**Key Points and Considerations:**\n",
    "\n",
    "*   **Data Type Mapping:** Be aware of data type mapping between MariaDB and Redshift. DMS will do its best to convert, but you should review the mapping rules.\n",
    "*   **Initial Load Time:** The initial full load can take a long time if your database is large.\n",
    "*   **Schema Evolution:** Be mindful that DMS can manage schema changes, but certain types of changes might require manually adjusting the task.\n",
    "*   **Troubleshooting:** DMS logs are valuable for troubleshooting issues, so keep an eye on them.\n",
    "*   **Task Configuration:** Review the DMS task configuration carefully to avoid issues.\n",
    "*   **Resource Usage:** Monitor the resources used by your replication instance. Adjust its size if required.\n",
    "*   **Redshift Table Design:** DMS does a basic table structure conversion based on the schema, you might have to manually tune the table structures in Redshift to optimize for analytics.\n",
    "\n",
    "**In Summary:**\n",
    "\n",
    "Using DMS, you can perform a one-time full data load from MariaDB to Redshift Serverless and then replicate changes in real time. This approach simplifies the process and provides a robust and manageable solution for your synchronization needs. Remember to monitor and troubleshoot your setup regularly to ensure everything is running smoothly. Always follow the best security practices for AWS IAM.\n",
    "\n",
    "By following these steps, you should be able to set up a reliable and efficient data synchronization process between your MariaDB database on an EC2 instance and AWS Redshift Serverless using AWS Database Migration Service. Good luck!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8065e3-1372-4f0f-b994-d08c0013ba3a",
   "metadata": {},
   "source": [
    "---\n",
    "## Alternative Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825f97f0-b794-43c7-a9f5-21a91e8082c9",
   "metadata": {},
   "source": [
    "You're right to seek a more automated solution for schema compatibility. While a completely \"magic\" solution that handles every edge case is difficult to achieve, we can significantly reduce the manual effort using libraries, tools, and strategies. Here's a breakdown of how to approach this, moving beyond manual transformations:\n",
    "\n",
    "**1. Leveraging SQLAlchemy's Type Reflection:**\n",
    "\n",
    "*   SQLAlchemy is excellent for introspection, allowing you to get precise schema information from the source database.\n",
    "*   It can also help define a target schema automatically, but you'll still need some logic for mapping between fundamentally different database systems.\n",
    "\n",
    "**Revised Schema Fetching (Using SQLAlchemy for MariaDB):**\n",
    "\n",
    "```python\n",
    "from sqlalchemy import create_engine, MetaData\n",
    "from sqlalchemy.schema import Table, Column\n",
    "from sqlalchemy.types import *\n",
    "\n",
    "\n",
    "def get_mariadb_schema_sqlalchemy(mariadb_engine, table_name):\n",
    "    \"\"\"\n",
    "    Fetches the schema of a table from MariaDB using SQLAlchemy.\n",
    "    \"\"\"\n",
    "    metadata = MetaData()\n",
    "    try:\n",
    "        # Reflect the table from the database\n",
    "        table = Table(table_name, metadata, autoload_with=mariadb_engine)\n",
    "\n",
    "        schema = []\n",
    "        for column in table.columns:\n",
    "            col_name = column.name\n",
    "            col_type = column.type\n",
    "            col_nullable = column.nullable\n",
    "            col_default = column.default.arg if column.default else None\n",
    "\n",
    "            # Convert SQLAlchemy type to generic names\n",
    "            if isinstance(col_type, (Integer, SmallInteger, BigInteger)):\n",
    "                redshift_type = \"INTEGER\" # Or adjust size as needed\n",
    "            elif isinstance(col_type, (Float, Numeric)):\n",
    "                redshift_type = \"DOUBLE PRECISION\" # Adjust as needed (FLOAT vs DOUBLE)\n",
    "            elif isinstance(col_type, String):\n",
    "                if isinstance(col_type, CHAR):\n",
    "                  redshift_type = f\"CHAR({col_type.length})\" if col_type.length else 'CHAR'\n",
    "                elif isinstance(col_type, VARCHAR):\n",
    "                  redshift_type = f\"VARCHAR({col_type.length})\" if col_type.length else 'VARCHAR'\n",
    "                elif isinstance(col_type, Text):\n",
    "                  redshift_type = \"TEXT\"\n",
    "                else:\n",
    "                  redshift_type = \"VARCHAR\"\n",
    "            elif isinstance(col_type, (Date, DateTime, Time)):\n",
    "                redshift_type = str(col_type).upper()  # DATE, DATETIME, TIME\n",
    "            elif isinstance(col_type, LargeBinary):\n",
    "                redshift_type = \"BYTEA\"\n",
    "            elif isinstance(col_type, Enum):\n",
    "                redshift_type = \"VARCHAR\"\n",
    "            elif isinstance(col_type, Boolean):\n",
    "              redshift_type = 'BOOLEAN'\n",
    "            else:\n",
    "              redshift_type = \"VARCHAR\"\n",
    "\n",
    "            schema.append({\n",
    "                'name': col_name,\n",
    "                'type': redshift_type,\n",
    "                'nullable': col_nullable,\n",
    "                'default': col_default\n",
    "            })\n",
    "        return schema\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error getting schema for table '{table_name}': {e}\")\n",
    "        raise\n",
    "\n",
    "def get_mariadb_engine():\n",
    "  \"\"\"Creates and returns a SQLAlchemy engine for MariaDB.\"\"\"\n",
    "  try:\n",
    "    mariadb_uri = f\"mysql+mysqlconnector://{MARIADB_USER}:{MARIADB_PASSWORD}@{MARIADB_HOST}/{MARIADB_DATABASE}\"\n",
    "    engine = create_engine(mariadb_uri)\n",
    "    return engine\n",
    "  except SQLAlchemyError as e:\n",
    "      logging.error(f\"Error connecting to MariaDB: {e}\")\n",
    "      raise\n",
    "\n",
    "```\n",
    "\n",
    "**Changes:**\n",
    "\n",
    "1.  **SQLAlchemy Engine:** The `get_mariadb_engine()` function now creates a SQLAlchemy engine for MariaDB.\n",
    "2.  **Schema Fetching:** `get_mariadb_schema_sqlalchemy()` uses SQLAlchemy reflection to get more information about each column, including its type, nullability, and default value.\n",
    "3.  **SQLAlchemy Types:** It uses SQLAlchemy's type classes (`Integer`, `String`, `DateTime`, etc.) to determine the general type of the column and uses these to transform to Redshift compatible types.\n",
    "4.  **Less Manual Mapping:**  The code translates SQLAlchemy column types into a redshift compatible types.\n",
    "\n",
    "**2. Handling Complex Mappings and Edge Cases:**\n",
    "\n",
    "*   **Explicit Handling:** The above changes reduce manual work by using SQLAlchemy's type system, but you will still need explicit type conversion for custom types.\n",
    "*   **Logging and Validation:**  Add logging to track which data types are being mapped to what, and potentially write checks that raise alerts if a type not supported is used.\n",
    "*   **User-Defined Mapping:** If you have specific types or custom logic, implement it through a lookup function or similar approach to add flexibility.\n",
    "\n",
    "**3. Alternatives & Tools:**\n",
    "\n",
    "*   **AWS Schema Conversion Tool (SCT):** A very powerful tool from AWS specifically designed for database migrations. It analyzes your source schema and automatically suggests the best mapping to your target Redshift schema. It handles a lot of complex scenarios well and should be the go-to solution for production migrations.\n",
    "*   **ETL Tools:** Use specialized ETL (Extract, Transform, Load) tools that often have built-in schema mapping and transformations, such as:\n",
    "    *   Apache Airflow with the Redshift hook\n",
    "    *   Talend\n",
    "    *   Informatica\n",
    "    *   AWS Glue\n",
    "*   **Database Specific Migration Tools:** MariaDB and Redshift might offer command-line tools or scripts for schema export/import (although these generally do not perform the compatibility step themselves).\n",
    "*   **Customizable Data Quality Framework:** Build your custom data quality framework to validate your data after the migration and identify mapping issues.\n",
    "\n",
    "**Best Approach:**\n",
    "\n",
    "1.  **Start with AWS SCT:** If you have access to AWS SCT, start there. It's a more comprehensive and reliable option than scripting everything from scratch.\n",
    "2.  **Combine SCT and Script:** Even if you use SCT, keep a Python script for data transfer and any post-migration verification.\n",
    "3.  **SQLAlchemy Introspection:** The SQLAlchemy method is good for learning about each table schema and making some automatic transformation.\n",
    "4.  **Handle Edge Cases:** Manually handle issues in your data types, especially with custom or complex types.\n",
    "5.  **Test Extensively:** Thoroughly test your migration process.\n",
    "\n",
    "**Key Takeaway:**\n",
    "\n",
    "While you cannot eliminate manual intervention entirely, you can greatly reduce it by using the following:\n",
    "\n",
    "*   SQLAlchemy for more advanced schema introspection.\n",
    "*   Specialized tools like AWS SCT for schema conversions.\n",
    "*   ETL tools for end-to-end solutions.\n",
    "*   Robust logging and validation checks.\n",
    "\n",
    "The code above provides the basis for automating type mapping, but you must consider the above ideas for a production-ready schema migration pipeline.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
