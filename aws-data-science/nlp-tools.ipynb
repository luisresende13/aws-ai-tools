{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "494d4f6a-73cf-4290-9dcf-e5b4749dd26e",
   "metadata": {},
   "source": [
    "AWS offers a growing suite of tools and services specifically designed for **Large Language Models (LLMs)**, **Retrieval-Augmented Generation (RAG)**, **AI agents**, and related workflows. Below is a breakdown of the key tools and how they fit into these advanced AI/ML use cases:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. AWS Tools for LLMs**\n",
    "#### **Managed LLM Access & Fine-Tuning**\n",
    "- **Amazon Bedrock**:  \n",
    "  - Fully managed service to access foundation models (FMs) like **Anthropic Claude**, **Cohere**, **Meta Llama**, **Mistral**, and AWS Titan.  \n",
    "  - Supports fine-tuning, RAG integration, and guardrails for safe AI.  \n",
    "  - Use cases: Chatbots, content generation, summarization.  \n",
    "\n",
    "- **SageMaker JumpStart**:  \n",
    "  - Pre-built LLMs (e.g., Falcon, Llama-2, Mistral) with one-click deployment.  \n",
    "  - Includes notebooks for fine-tuning and inference.  \n",
    "\n",
    "- **SageMaker Training/Inference**:  \n",
    "  - Custom training and deployment of open-source LLMs (e.g., using Hugging Face libraries).  \n",
    "  - Optimized infrastructure (e.g., AWS Trainium/Inferentia chips) for cost-effective scaling.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. RAG (Retrieval-Augmented Generation) Tools**\n",
    "RAG combines LLMs with external knowledge retrieval. AWS services to build RAG pipelines:\n",
    "- **Knowledge Bases for Amazon Bedrock**:  \n",
    "  - Managed RAG solution: Ingest data (PDFs, docs, etc.) into a vector database, retrieve context, and augment LLM prompts.  \n",
    "  - Integrates with **OpenSearch** or **Pinecone** for vector storage.  \n",
    "\n",
    "- **Amazon Kendra**:  \n",
    "  - Intelligent enterprise search service to retrieve structured/unstructured data for RAG workflows.  \n",
    "\n",
    "- **Vector Databases**:  \n",
    "  - **Amazon OpenSearch** (with k-NN plugin) for vector similarity search.  \n",
    "  - **Amazon Aurora PostgreSQL** (with `pgvector` extension).  \n",
    "  - **Amazon MemoryDB** (Redis-compatible) for low-latency caching.  \n",
    "\n",
    "- **AWS Glue & Lake Formation**:  \n",
    "  - Prepare and catalog data for RAG (e.g., cleaning text, extracting metadata).\n",
    "\n",
    "---\n",
    "\n",
    "### **3. AI Agent Tools**\n",
    "Agents use LLMs to autonomously plan and execute tasks. AWS services for building agents:\n",
    "- **Agents for Amazon Bedrock**:  \n",
    "  - Pre-built agents that orchestrate tasks (e.g., answering questions by querying databases, APIs, or knowledge bases).  \n",
    "  - Supports action groups (Lambda functions, APIs) for task execution.  \n",
    "\n",
    "- **AWS Step Functions**:  \n",
    "  - Coordinate multi-step workflows (e.g., chaining LLM calls, data retrieval, and business logic).  \n",
    "\n",
    "- **Amazon Lex**:  \n",
    "  - Build conversational interfaces (chatbots) integrated with LLMs.  \n",
    "\n",
    "- **AWS Lambda**:  \n",
    "  - Serverless functions to trigger agent actions (e.g., calling APIs, processing data).  \n",
    "\n",
    "- **Amazon EventBridge**:  \n",
    "  - Event-driven orchestration for agent-based systems.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Specialized Tools for LLM Workflows**\n",
    "- **Amazon CodeWhisperer**:  \n",
    "  - AI-powered code companion (similar to GitHub Copilot) for developers.  \n",
    "\n",
    "- **Amazon Q**:  \n",
    "  - Generative AI assistant for AWS, DevOps, and business intelligence (e.g., troubleshooting, data insights).  \n",
    "\n",
    "- **Amazon Titan Embeddings**:  \n",
    "  - AWS’s proprietary text embedding model (via Bedrock) for RAG and semantic search.  \n",
    "\n",
    "- **Amazon SageMaker Clarify**:  \n",
    "  - Detect bias, explain predictions, and evaluate LLM outputs.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. MLOps for LLMs**\n",
    "- **SageMaker Pipelines**:  \n",
    "  - Orchestrate LLM fine-tuning, evaluation, and deployment.  \n",
    "- **SageMaker Model Registry**:  \n",
    "  - Catalog and version LLMs.  \n",
    "- **SageMaker Model Monitor**:  \n",
    "  - Track model drift and performance in production.  \n",
    "- **SageMaker Inference Recommender**:  \n",
    "  - Optimize LLM deployment configurations (instance types, scaling).\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Security & Governance**\n",
    "- **AWS IAM**:  \n",
    "  - Control access to LLMs and data sources.  \n",
    "- **AWS KMS**:  \n",
    "  - Encrypt sensitive data used in RAG pipelines.  \n",
    "- **Amazon Bedrock Guardrails**:  \n",
    "  - Filter harmful content and enforce safety policies.  \n",
    "- **AWS CloudTrail**:  \n",
    "  - Audit API calls (e.g., Bedrock, SageMaker).\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Workflow: Build a RAG-Powered Agent**\n",
    "1. **Ingest Data**:  \n",
    "   - Store documents in S3; catalog with AWS Glue.  \n",
    "2. **Create Knowledge Base**:  \n",
    "   - Use **Bedrock Knowledge Bases** to generate embeddings and store in OpenSearch.  \n",
    "3. **Build Agent**:  \n",
    "   - Use **Agents for Bedrock** to define actions (e.g., query OpenSearch via Lambda).  \n",
    "4. **Deploy LLM**:  \n",
    "   - Use Bedrock’s Claude or SageMaker JumpStart’s Llama-2 for generation.  \n",
    "5. **Orchestrate**:  \n",
    "   - Use Step Functions to chain retrieval, generation, and post-processing.  \n",
    "6. **Monitor**:  \n",
    "   - Track performance with SageMaker Model Monitor and CloudTrail.\n",
    "\n",
    "---\n",
    "\n",
    "### **Learning Resources**\n",
    "- [Amazon Bedrock Docs](https://docs.aws.amazon.com/bedrock/)  \n",
    "- [Build RAG with Bedrock Knowledge Bases](https://aws.amazon.com/blogs/machine-learning/implementing-retrieval-augmented-generation-with-amazon-bedrock/)  \n",
    "- [Agents for Amazon Bedrock Tutorial](https://docs.aws.amazon.com/bedrock/latest/userguide/agents.html)  \n",
    "\n",
    "AWS continues to expand its LLM/Generative AI tooling, making it easier to build secure, scalable, and cost-effective AI applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
