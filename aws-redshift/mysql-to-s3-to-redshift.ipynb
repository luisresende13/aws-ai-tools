{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "710437d9-46c1-4242-85ef-e27ee46bb833",
   "metadata": {},
   "source": [
    "# Migrating tables from a remote MySQL server to Redshift Serverless using Python\n",
    "\n",
    "### **1. Prerequisites:**\n",
    "\n",
    "* **Python Environment:** You need Python 3.6 or higher installed, preferably with virtual environments.\n",
    "* **Required Libraries:** Install the following Python packages:\n",
    "    * `mysql-connector-python`: For connecting to the MySQL server.\n",
    "    * `psycopg2`: For connecting to Redshift Serverless.\n",
    "    * `boto3`: For AWS interactions.\n",
    "    * `tqdm` (optional): For progress bars.\n",
    "\n",
    "    ```bash\n",
    "    pip install mysql-connector-python psycopg2 boto3 tqdm\n",
    "    ```\n",
    "* **AWS Credentials:** Ensure you have the necessary AWS credentials configured (e.g., via environment variables, `~/.aws/credentials`, or an IAM role).\n",
    "* **Redshift Serverless Setup:** You need a Redshift Serverless endpoint created and its connection details (workgroup name, database name, etc.).\n",
    "* **MySQL Server Connection Details:** You need the hostname, username, password, and database name of the MySQL server.\n",
    "* **Network Connectivity:** Ensure that your environment can connect to both the MySQL server and Redshift Serverless.\n",
    "* **S3 Bucket:** An S3 bucket is required to temporarily store the data extracted from MySQL.\n",
    "\n",
    "### **2. Understanding the Process:**\n",
    "\n",
    "The migration process can be summarized as follows:\n",
    "\n",
    "1. **Connect to MySQL:** Establish a connection to the remote MySQL server.\n",
    "2. **Extract Data:** Query the desired tables from MySQL and retrieve the data.\n",
    "3. **Convert to CSV:**  Transform the data into CSV format (or other suitable format like Parquet).\n",
    "4. **Upload to S3:** Upload the generated CSV files to an S3 bucket.\n",
    "5. **Connect to Redshift:** Connect to your Redshift Serverless environment.\n",
    "6. **Create Tables in Redshift:** Create the corresponding tables in Redshift based on the MySQL schema.\n",
    "7. **Copy Data from S3:** Use Redshift's `COPY` command to load the data from the S3 bucket into the Redshift tables.\n",
    "8. **Clean Up (Optional):** Optionally, delete the temporary files from the S3 bucket.\n",
    "\n",
    "### **3. Python Implementation:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e48691-52a3-4692-aa1f-6cb6e3793ec0",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "786b4699-040b-4dc1-a019-b4d970a73bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MYSQL_HOST = \"<ec2-external-ip>\"\n",
    "MYSQL_PORT = \"<mysql-port>\"\n",
    "MYSQL_USER = \"<database-user>\"\n",
    "MYSQL_PASSWORD = \"<database-password>\"\n",
    "MYSQL_DATABASE = \"<database-name>\"\n",
    "MYSQL_TABLE = \"<table-name>\"\n",
    "\n",
    "REDSHIFT_SERVERLESS_WORKGROUP = \"<workgroup-name>\"  # e.g., \"my-workgroup\"\n",
    "REDSHIFT_DATABASE = \"<database-name-(default: 'dev')>\"\n",
    "REDSHIFT_USER = \"<database-user>\"\n",
    "REDSHIFT_PASSWORD = \"<database-password>\"\n",
    "AWS_REGION = \"<aws-region>\"  # e.g., \"us-east-1\"\n",
    "S3_BUCKET = \"<bucket-name>\"\n",
    "S3_FOLDER = \"<folder-name>\"  # Folder for storing temporary files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9ddd62-5540-4b76-b69f-349bcd076e4d",
   "metadata": {},
   "source": [
    "#### Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab0179a-7b14-4916-b321-dd5090dad6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mysql\n",
    "# !pip install mysql.connector\n",
    "# !pip install psycopg2\n",
    "# !pip install redshift_connector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a4849e-c38e-482a-b28e-e81960980329",
   "metadata": {},
   "source": [
    "#### Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "59bfee74-58f2-42c8-b4b4-3c9296379d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import psycopg2\n",
    "import boto3\n",
    "import csv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from io import StringIO  # Import StringIO\n",
    "\n",
    "def get_redshift_endpoint():\n",
    "    redshift_client = boto3.client('redshift-serverless', region_name=AWS_REGION)\n",
    "    response = redshift_client.get_workgroup(workgroupName=REDSHIFT_SERVERLESS_WORKGROUP)\n",
    "    endpoint = response['workgroup']['endpoint']['address']\n",
    "    port = response['workgroup']['endpoint']['port']\n",
    "    return f\"{endpoint}:{port}\"\n",
    "\n",
    "def get_mysql_schema(cursor, table_name):\n",
    "  \"\"\"\n",
    "  Retrieves the schema (column names and types) of a MySQL table.\n",
    "  \"\"\"\n",
    "  cursor.execute(f\"SHOW COLUMNS FROM `{table_name}`\")\n",
    "  columns = cursor.fetchall()\n",
    "  schema = []\n",
    "  for column in columns:\n",
    "    schema.append({\n",
    "        'name': column[0],\n",
    "        'type': column[1],\n",
    "        'is_nullable': column[2],\n",
    "        'is_key': column[3]\n",
    "    })\n",
    "  return schema\n",
    "\n",
    "def create_redshift_table_statement(schema, table_name):\n",
    "    \"\"\"\n",
    "    Generates a CREATE TABLE statement for Redshift based on the MySQL schema.\n",
    "    \"\"\"\n",
    "    columns = []\n",
    "    for column in schema:\n",
    "        redshift_type = convert_mysql_type_to_redshift(column['type'])\n",
    "        if redshift_type:\n",
    "            nullable_str = \"NULL\" if column['is_nullable'] == 'YES' else \"NOT NULL\"\n",
    "            columns.append(f\"    \\\"{column['name']}\\\" {redshift_type} {nullable_str}\")\n",
    "        else:\n",
    "           print(f\"Column {column['name']} has an unsupport data type {column['type']}, skipping\")\n",
    "\n",
    "    if not columns:\n",
    "       return None # skip creation if no columns were converted\n",
    "    statement = f\"CREATE TABLE IF NOT EXISTS \\\"{table_name}\\\" (\\n\"\n",
    "    statement += \",\\n\".join(columns)\n",
    "    statement += \"\\n)\"\n",
    "    return statement\n",
    "\n",
    "def convert_mysql_type_to_redshift(mysql_type):\n",
    "    \"\"\"\n",
    "    Converts a MySQL data type to a corresponding Redshift data type.\n",
    "    \"\"\"\n",
    "    mysql_type = mysql_type.lower()\n",
    "    if 'int' in mysql_type:\n",
    "        if 'tinyint' in mysql_type:\n",
    "            return 'SMALLINT'\n",
    "        elif 'smallint' in mysql_type:\n",
    "            return 'SMALLINT'\n",
    "        elif 'mediumint' in mysql_type:\n",
    "            return 'INTEGER'\n",
    "        elif 'bigint' in mysql_type:\n",
    "            return 'BIGINT'\n",
    "        else:\n",
    "            return 'INTEGER'\n",
    "    elif 'decimal' in mysql_type or 'numeric' in mysql_type:\n",
    "      return 'DECIMAL'\n",
    "    elif 'float' in mysql_type:\n",
    "        return 'FLOAT'\n",
    "    elif 'double' in mysql_type or 'real' in mysql_type:\n",
    "      return 'DOUBLE PRECISION'\n",
    "    elif 'varchar' in mysql_type or 'char' in mysql_type or 'text' in mysql_type:\n",
    "        return 'VARCHAR(65535)' # Max varchar allowed in redshift is 65535\n",
    "    elif 'date' in mysql_type:\n",
    "      return 'DATE'\n",
    "    elif 'datetime' in mysql_type or 'timestamp' in mysql_type:\n",
    "        return 'TIMESTAMP'\n",
    "    elif 'boolean' in mysql_type or 'tinyint(1)' in mysql_type:\n",
    "      return 'BOOLEAN'\n",
    "    # Add more type conversions as needed\n",
    "    return None\n",
    "\n",
    "def fetch_mysql_data(cursor, table_name):\n",
    "    \"\"\"Fetches all data from a MySQL table.\"\"\"\n",
    "    cursor.execute(f\"SELECT * FROM `{table_name}`\")\n",
    "    return cursor.fetchall()\n",
    "\n",
    "def upload_data_to_s3(s3_client, data, table_name):\n",
    "    \"\"\"Uploads CSV data to S3.\"\"\"\n",
    "    s3_file_path = f\"{S3_FOLDER}/{table_name}.csv\"\n",
    "    csv_buffer = StringIO()\n",
    "    csv_writer = csv.writer(csv_buffer, quoting=csv.QUOTE_NONNUMERIC) # CSV supports quoted values to handle commas within the data.\n",
    "    csv_writer.writerows(data)\n",
    "    try:\n",
    "        s3_client.put_object(\n",
    "            Bucket=S3_BUCKET,\n",
    "            Key=s3_file_path,\n",
    "            Body=csv_buffer.getvalue()\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading to S3: {e}\")\n",
    "        return False\n",
    "\n",
    "    print(f\"Data for '{table_name}' uploaded to s3://{S3_BUCKET}/{s3_file_path}\")\n",
    "    return True\n",
    "\n",
    "def copy_data_to_redshift(conn, table_name, s3_file_path, redshift_user, redshift_password):\n",
    "    \"\"\"Copies data from S3 to Redshift using COPY command.\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        copy_statement = f\"\"\"\n",
    "          COPY \"{table_name}\" FROM 's3://{S3_BUCKET}/{s3_file_path}'\n",
    "          CREDENTIALS 'aws_iam_role={get_iam_role()}'\n",
    "          FORMAT AS CSV\n",
    "          IGNOREHEADER 0\n",
    "          QUOTE AS '\"'\n",
    "          REGION AS '{AWS_REGION}';\n",
    "        \"\"\"\n",
    "        cursor.execute(copy_statement)\n",
    "        conn.commit()\n",
    "        print(f\"Data copied to Redshift table '{table_name}'\")\n",
    "    except Exception as e:\n",
    "      print(f\"Error copying data to Redshift: {e}\")\n",
    "\n",
    "def get_iam_role():\n",
    "    \"\"\"\n",
    "        Gets IAM role associated with this ec2 instance or lambda function\n",
    "        Returns:\n",
    "          role: The iam role ARN\n",
    "    \"\"\"\n",
    "    iam_client = boto3.client('iam')\n",
    "    try:\n",
    "        # lambda_role\n",
    "        role = boto3.client('sts').get_caller_identity().get('Arn')\n",
    "        role_name = role.split('/')[-1]\n",
    "        role = iam_client.get_role(RoleName=role_name)['Role']['Arn']\n",
    "\n",
    "    except Exception as e:\n",
    "        # ec2_role\n",
    "        role = boto3.client('sts').get_caller_identity().get('Arn')\n",
    "        role_name = role.split('/')[-2]\n",
    "        role = iam_client.get_role(RoleName=role_name)['Role']['Arn']\n",
    "    return role\n",
    "\n",
    "def migrate_table(mysql_conn, redshift_conn, s3_client, table_name):\n",
    "    \"\"\"Migrates a single table from MySQL to Redshift.\"\"\"\n",
    "    mysql_cursor = mysql_conn.cursor()\n",
    "\n",
    "    # Get MySQL table schema\n",
    "    print(f\"Fetching schema for table '{table_name}'\")\n",
    "    schema = get_mysql_schema(mysql_cursor, table_name)\n",
    "    if not schema:\n",
    "      print(f\"Skipping table {table_name}, error during schema fetch\")\n",
    "      return\n",
    "\n",
    "    # Create Redshift table\n",
    "    create_table_statement = create_redshift_table_statement(schema, table_name)\n",
    "    if not create_table_statement:\n",
    "        print(f\"Skipping table {table_name}, error during table creation statement generation\")\n",
    "        return\n",
    "    try:\n",
    "        redshift_cursor = redshift_conn.cursor()\n",
    "        redshift_cursor.execute(create_table_statement)\n",
    "        redshift_conn.commit()\n",
    "        print(f\"Redshift table '{table_name}' created successfully\")\n",
    "    except Exception as e:\n",
    "      print(f\"Error creating Redshift table: {e}\")\n",
    "      return\n",
    "\n",
    "    # Fetch data from MySQL\n",
    "    print(f\"Fetching data for table '{table_name}'\")\n",
    "    data = fetch_mysql_data(mysql_cursor, table_name)\n",
    "    if not data:\n",
    "      print(f\"Skipping table {table_name}, no data returned\")\n",
    "      return\n",
    "\n",
    "    # Upload data to S3\n",
    "    if not upload_data_to_s3(s3_client, data, table_name):\n",
    "      print(f\"Skipping table {table_name}, error during s3 upload\")\n",
    "      return\n",
    "\n",
    "    # Copy data to Redshift\n",
    "    s3_file_path = f\"{S3_FOLDER}/{table_name}.csv\"\n",
    "    copy_data_to_redshift(redshift_conn, table_name, s3_file_path, REDSHIFT_USER, REDSHIFT_PASSWORD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0818f13-f72d-44ae-b9ce-34d00995268f",
   "metadata": {},
   "source": [
    "#### MySQL Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c077dfa9-f723-4377-893d-55d3becc8135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to MySQL...\n",
      "Connected to MySQL successfully.\n"
     ]
    }
   ],
   "source": [
    "# Connect to MySQL\n",
    "print(\"Connecting to MySQL...\")\n",
    "mysql_conn = mysql.connector.connect(\n",
    "    host=MYSQL_HOST,\n",
    "    port=MYSQL_PORT,\n",
    "    user=MYSQL_USER,\n",
    "    password=MYSQL_PASSWORD,\n",
    "    database=MYSQL_DATABASE\n",
    ")\n",
    "print(\"Connected to MySQL successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a714c76-4a08-41c6-b6da-cce90a025b19",
   "metadata": {},
   "source": [
    "#### Get Table Schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19142b3-94b6-4b8e-8e8d-a9d091d26479",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Migrates a single table from MySQL to Redshift.\"\"\"\n",
    "mysql_cursor = mysql_conn.cursor()\n",
    "\n",
    "# Get MySQL table schema\n",
    "print(f\"Fetching schema for table '{MYSQL_TABLE}'\")\n",
    "schema = get_mysql_schema(mysql_cursor, MYSQL_TABLE)\n",
    "if not schema:\n",
    "  print(f\"Skipping table {MYSQL_TABLE}, error during schema fetch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac5d0bc-8c32-4edc-9f25-848d9d62e2c0",
   "metadata": {},
   "source": [
    "#### Build \"Create Table\" Statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8baee639-e54a-448f-ba76-53b9dca432d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE IF NOT EXISTS \"user\" (\n",
      "    \"id\" INTEGER NOT NULL,\n",
      "    \"mob_id\" VARCHAR(65535) NOT NULL,\n",
      "    \"salutation\" VARCHAR(65535) NULL,\n",
      "    \"first_name\" VARCHAR(65535) NULL,\n",
      "    \"last_name\" VARCHAR(65535) NULL,\n",
      "    \"email\" VARCHAR(65535) NOT NULL,\n",
      "    \"temp_email\" VARCHAR(65535) NULL,\n",
      "    \"password\" VARCHAR(65535) NOT NULL,\n",
      "    \"phone\" VARCHAR(65535) NULL,\n",
      "    \"phone2\" VARCHAR(65535) NULL,\n",
      "    \"mfa_phone\" VARCHAR(65535) NULL,\n",
      "    \"mfa_temp_phone\" VARCHAR(65535) NULL,\n",
      "    \"mfa_phone_state\" SMALLINT NOT NULL,\n",
      "    \"fax\" VARCHAR(65535) NULL,\n",
      "    \"title\" VARCHAR(65535) NULL,\n",
      "    \"reset_password\" SMALLINT NULL,\n",
      "    \"current_project_id\" INTEGER NULL,\n",
      "    \"primary_contact\" SMALLINT NULL,\n",
      "    \"pic\" VARCHAR(65535) NULL,\n",
      "    \"license_exp\" DATE NULL,\n",
      "    \"company_id\" INTEGER NULL,\n",
      "    \"address_id\" INTEGER NULL,\n",
      "    \"reminder_key\" VARCHAR(65535) NULL,\n",
      "    \"reminder_exp\" DATE NULL,\n",
      "    \"iss_notification\" SMALLINT NULL,\n",
      "    \"activation_dt\" DATE NULL,\n",
      "    \"new_id\" INTEGER NULL,\n",
      "    \"old_id\" INTEGER NULL,\n",
      "    \"state\" SMALLINT NOT NULL,\n",
      "    \"signature\" VARCHAR(65535) NULL,\n",
      "    \"created_by_user_id\" INTEGER NOT NULL,\n",
      "    \"created\" TIMESTAMP NULL,\n",
      "    \"modified\" TIMESTAMP NULL\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create Redshift table\n",
    "create_table_statement = create_redshift_table_statement(schema, MYSQL_TABLE)\n",
    "\n",
    "print(create_table_statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60050a21-8f22-493a-807f-536e1e97bead",
   "metadata": {},
   "source": [
    "#### Fetch Redshift Endpoint Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d05f6d85-8b54-473c-b2e9-0fbfcddca44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Redshift Serverless endpoint...\n",
      "Redshift Serverless endpoint: test-temp.654395624723.us-east-1.redshift-serverless.amazonaws.com:5439\n"
     ]
    }
   ],
   "source": [
    "# Get redshift connection info\n",
    "print(\"Fetching Redshift Serverless endpoint...\")\n",
    "redshift_endpoint = get_redshift_endpoint()\n",
    "print(f\"Redshift Serverless endpoint: {redshift_endpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b407946-faf2-4ed2-96c2-405523525faf",
   "metadata": {},
   "source": [
    "#### Redshift Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53566e2a-f9c0-46ec-8af1-e0a8cbecb553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import redshift_connector\n",
    "\n",
    "redshift_conn = redshift_connector.connect(\n",
    "    host=redshift_endpoint.split(':')[0],\n",
    "    port=redshift_endpoint.split(':')[1],\n",
    "    database=REDSHIFT_DATABASE,\n",
    "    user=REDSHIFT_USER,\n",
    "    password=REDSHIFT_PASSWORD,\n",
    "    access_key_id='your secret key id',\n",
    "    secret_access_key='your secret key',\n",
    ")\n",
    "\n",
    "# ALTERNATIVE METHOD:\n",
    "\n",
    "# import psycopg2\n",
    "\n",
    "# print(\"Connecting to Redshift Serverless...\")\n",
    "# redshift_conn = psycopg2.connect(\n",
    "#     host=redshift_endpoint.split(':')[0],\n",
    "#     port=redshift_endpoint.split(':')[1],\n",
    "#     database=REDSHIFT_DATABASE,\n",
    "#     user=REDSHIFT_USER,\n",
    "#     password=REDSHIFT_PASSWORD\n",
    "# )\n",
    "# print(\"Connected to Redshift Serverless successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53340fce-4b70-478f-9aea-d04f5cb6f35a",
   "metadata": {},
   "source": [
    "### Upload Tables to S3 and Migrate to Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ed5877-5e4b-441a-997b-9280b6dc2477",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    # Connect to S3\n",
    "    s3_client = boto3.client('s3', region_name=AWS_REGION)\n",
    "\n",
    "    # List tables from MySQL\n",
    "    mysql_cursor = mysql_conn.cursor()\n",
    "    mysql_cursor.execute(\"SHOW TABLES\")\n",
    "    tables = [table[0] for table in mysql_cursor.fetchall()]\n",
    "    print(f\"Tables found in MySQL: {tables}\")\n",
    "\n",
    "    # Migrate each table\n",
    "    for table_name in tqdm(tables, desc=\"Migrating Tables\"):\n",
    "        print(f\"Migrating table: {table_name}\")\n",
    "        migrate_table(mysql_conn, redshift_conn, s3_client, table_name)\n",
    "        print(f\"Finished migrating table: {table_name}\")\n",
    "\n",
    "    print(\"Migration completed successfully!\")\n",
    "    # cleanup S3 folder (Optional)\n",
    "    # for table_name in tables:\n",
    "    #     s3_file_path = f\"{S3_FOLDER}/{table_name}.csv\"\n",
    "    #     s3_client.delete_object(Bucket=S3_BUCKET, Key=s3_file_path)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    if 'mysql_conn' in locals() and mysql_conn.is_connected():\n",
    "       mysql_conn.close()\n",
    "    if 'redshift_conn' in locals() and redshift_conn.closed == 0 :\n",
    "        redshift_conn.close()\n",
    "    print(\"Connections closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292036d0-741d-4343-adbc-1c65a6f8dba0",
   "metadata": {},
   "source": [
    "**4. Explanation:**\n",
    "\n",
    "*   **Configuration:** The code begins by setting up the necessary configuration variables for connecting to MySQL, Redshift Serverless, and S3.\n",
    "*   **Connection Functions:** The code includes helper functions to connect to MySQL, retrieve schema, generate table create statement, convert type, fetch data, upload to S3, copy to redshift, and retrieve the iam role.\n",
    "*   **Table Migration Loop:** The `main()` function iterates through the MySQL tables, calling the `migrate_table()` function for each one.\n",
    "*   **`migrate_table()` Function:** This function orchestrates the migration of a single table by:\n",
    "    *   Fetching the table schema from MySQL.\n",
    "    *   Creating the corresponding table in Redshift based on the retrieved schema.\n",
    "    *   Fetching the data from MySQL.\n",
    "    *   Uploading the data as CSV files to S3.\n",
    "    *   Copying the data from S3 to Redshift using the `COPY` command.\n",
    "*  **Type Conversion:** The `convert_mysql_type_to_redshift` provides the mapping between mysql datatypes and redshift. More type conversions can be added here.\n",
    "*   **Error Handling:** The code includes `try-except` blocks for error handling during connection, table creation, data extraction, S3 upload, and Redshift copy.\n",
    "*   **IAM Role:** The `get_iam_role()` function helps to automatically fetch the IAM role attached to your instance or lambda functions for S3 access.\n",
    "*  **Quoting:** CSV data is handled by quoting all values to ensure that commas within values doesn't create a parsing error.\n",
    "\n",
    "**5. Important Considerations:**\n",
    "\n",
    "*   **Large Tables:** For very large tables, consider using a more efficient approach, such as partitioning the data in MySQL and S3, and then loading it into Redshift using multiple `COPY` commands in parallel.\n",
    "*   **Data Types:** Carefully check if the type mappings are complete and correct for your data.  You may need to expand the type conversion logic.\n",
    "*   **Error Handling:** Implement more comprehensive error handling and logging.\n",
    "*   **Security:** Securely handle your passwords and AWS credentials.\n",
    "*   **Network Configuration:** Ensure the proper network security group/firewall settings are in place to allow connections between MySQL, the script execution environment, and Redshift.\n",
    "*   **Performance Optimization:** Adjust batch sizes, connection parameters, and other performance settings as needed for your environment.\n",
    "*   **Incremental Loads:** For incremental data loads, you will need to implement logic to detect new or updated data in your MySQL database. Consider using timestamps, change data capture, or other methods for tracking changes.\n",
    "* **Schema Changes:** This script assumes the same schema for MySQL and Redshift. Consider implementing schema evolution logic if the schemas are not the same.\n",
    "* **Key Handling:** Primary keys, foreign keys, and other key constraints will need to be migrated separately if required.\n",
    "* **Quoting:** Consider if you have non-ascii characters or control characters in your data that need to be handled using advanced encoding techniques.\n",
    "\n",
    "**6. How to Run the Code:**\n",
    "\n",
    "1.  Replace placeholder values in the configuration section with your actual values.\n",
    "2.  Save the code as a Python file (e.g., `mysql_to_redshift.py`).\n",
    "3.  Run the script from your terminal:\n",
    "\n",
    "    ```bash\n",
    "    python mysql_to_redshift.py\n",
    "    ```\n",
    "\n",
    "This enhanced explanation and code example will help you migrate your tables from MySQL to Redshift Serverless. Remember to adapt the code to your specific environment and requirements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
